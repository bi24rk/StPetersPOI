{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bi24rk/StPetersPOI/blob/main/nutrition_fitness_tracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGIv_jdB5Q20",
        "outputId": "f32a40e2-62a5-4d82-b005-3b9f08d384f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shape: (5000, 30)\n",
            "\n",
            "Descriptive Statistics:\n",
            "                Age    Height_cm   Weight_kg          BMI  \\\n",
            "count  5000.000000  5000.000000  5000.00000  5000.000000   \n",
            "mean     48.805600   174.244000    84.36620    28.353134   \n",
            "std      17.906991    14.229173    20.18103     8.297745   \n",
            "min      18.000000   150.000000    50.00000    12.630000   \n",
            "25%      34.000000   162.000000    67.00000    21.850000   \n",
            "50%      49.000000   174.000000    84.00000    27.640000   \n",
            "75%      64.000000   186.000000   102.00000    33.812500   \n",
            "max      79.000000   199.000000   119.00000    52.890000   \n",
            "\n",
            "       Blood_Pressure_Systolic  Blood_Pressure_Diastolic  Cholesterol_Level  \\\n",
            "count              5000.000000               5000.000000        5000.000000   \n",
            "mean                133.982400                 89.735800         224.297800   \n",
            "std                  26.216215                 17.283025          42.918923   \n",
            "min                  90.000000                 60.000000         150.000000   \n",
            "25%                 111.000000                 75.000000         187.000000   \n",
            "50%                 133.000000                 90.000000         224.000000   \n",
            "75%                 157.000000                105.000000         261.000000   \n",
            "max                 179.000000                119.000000         299.000000   \n",
            "\n",
            "       Blood_Sugar_Level   Daily_Steps  Exercise_Frequency  Sleep_Hours  \\\n",
            "count         5000.00000   5000.000000         5000.000000  5000.000000   \n",
            "mean           159.33020   8458.922800            2.978200     7.019680   \n",
            "std             52.14943   3742.408853            2.001431     1.716133   \n",
            "min             70.00000   2004.000000            0.000000     4.000000   \n",
            "25%            114.00000   5278.750000            1.000000     5.600000   \n",
            "50%            160.00000   8452.000000            3.000000     7.000000   \n",
            "75%            204.00000  11671.750000            5.000000     8.500000   \n",
            "max            249.00000  14997.000000            6.000000    10.000000   \n",
            "\n",
            "       Caloric_Intake  Protein_Intake  Carbohydrate_Intake   Fat_Intake  \\\n",
            "count     5000.000000     5000.000000          5000.000000  5000.000000   \n",
            "mean      2347.350200      124.781800           248.590000    84.522600   \n",
            "std        659.880146       43.280037            86.535683    37.495091   \n",
            "min       1200.000000       50.000000           100.000000    20.000000   \n",
            "25%       1777.000000       87.000000           175.000000    52.000000   \n",
            "50%       2350.500000      126.000000           249.000000    85.000000   \n",
            "75%       2921.250000      162.000000           325.000000   116.000000   \n",
            "max       3499.000000      199.000000           399.000000   149.000000   \n",
            "\n",
            "       Recommended_Calories  Recommended_Protein  Recommended_Carbs  \\\n",
            "count           5000.000000          5000.000000        5000.000000   \n",
            "mean            2046.236000           129.287000         248.328600   \n",
            "std              671.380142            43.914252          91.152909   \n",
            "min              725.000000            40.000000          50.000000   \n",
            "25%             1478.000000            92.000000         173.000000   \n",
            "50%             2044.000000           129.000000         246.000000   \n",
            "75%             2621.000000           166.000000         324.000000   \n",
            "max             3372.000000           218.000000         447.000000   \n",
            "\n",
            "       Recommended_Fats  \n",
            "count       5000.000000  \n",
            "mean          89.029400  \n",
            "std           38.336083  \n",
            "min           10.000000  \n",
            "25%           56.000000  \n",
            "50%           89.000000  \n",
            "75%          121.000000  \n",
            "max          168.000000  \n",
            "Columns in dataset: ['Patient_ID', 'Age', 'Gender', 'Height_cm', 'Weight_kg', 'BMI', 'Chronic_Disease', 'Blood_Pressure_Systolic', 'Blood_Pressure_Diastolic', 'Cholesterol_Level', 'Blood_Sugar_Level', 'Genetic_Risk_Factor', 'Allergies', 'Daily_Steps', 'Exercise_Frequency', 'Sleep_Hours', 'Alcohol_Consumption', 'Smoking_Habit', 'Dietary_Habits', 'Caloric_Intake', 'Protein_Intake', 'Carbohydrate_Intake', 'Fat_Intake', 'Preferred_Cuisine', 'Food_Aversions', 'Recommended_Calories', 'Recommended_Protein', 'Recommended_Carbs', 'Recommended_Fats', 'Recommended_Meal_Plan']\n",
            "\n",
            "Testing on Small Dataset (100 rows):\n",
            "Logistic Regression (F1: 0.6150, Precision: 0.5946, Recall: 0.6500)\n",
            "SVM (F1: 0.5821, Precision: 0.5583, Recall: 0.6500)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest (F1: 0.6221, Precision: 0.5917, Recall: 0.7000)\n",
            "Decision Tree (F1: 0.9469, Precision: 0.9571, Recall: 0.9500)\n",
            "MLP (F1: 0.6706, Precision: 0.6571, Recall: 0.7000)\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8133 - loss: 0.7183\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN (F1: 0.4491, Precision: 0.4217, Recall: 0.5000)\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7828 - loss: 0.5318\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM (F1: 0.5364, Precision: 0.5167, Recall: 0.6000)\n",
            "\n",
            "Testing on Full Dataset:\n",
            "Logistic Regression (Full) (F1: 0.9739, Precision: 0.9748, Recall: 0.9740)\n",
            "SVM (Full) (F1: 0.9255, Precision: 0.9283, Recall: 0.9260)\n",
            "Random Forest (Full) (F1: 0.8825, Precision: 0.9190, Recall: 0.8930)\n",
            "Decision Tree (Full) (F1: 1.0000, Precision: 1.0000, Recall: 1.0000)\n",
            "MLP (Full) (F1: 0.9819, Precision: 0.9820, Recall: 0.9820)\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0011\n",
            "RNN (Full) (F1: 0.9850, Precision: 0.9850, Recall: 0.9850)\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 4.1035e-04\n",
            "LSTM (Full) (F1: 0.9789, Precision: 0.9791, Recall: 0.9790)\n",
            "\n",
            "Testing Model Performance Across Data Sizes:\n",
            "SVM (500 rows) (F1: 0.8224, Precision: 0.8546, Recall: 0.8200)\n",
            "Decision Tree (500 rows) (F1: 1.0000, Precision: 1.0000, Recall: 1.0000)\n",
            "Random Forest (500 rows) (F1: 0.8551, Precision: 0.8944, Recall: 0.8600)\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0124\n",
            "MLP (500 rows) (F1: 0.9105, Precision: 0.9123, Recall: 0.9100)\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9935 - loss: 0.1545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 37 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bb1dbac2d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 40 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7bb1dbac2d40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN (500 rows) (F1: 0.8619, Precision: 0.8669, Recall: 0.8600)\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0063\n",
            "LSTM (500 rows) (F1: 0.9496, Precision: 0.9496, Recall: 0.9500)\n",
            "SVM (1000 rows) (F1: 0.8385, Precision: 0.8447, Recall: 0.8400)\n",
            "Decision Tree (1000 rows) (F1: 0.9901, Precision: 0.9905, Recall: 0.9900)\n",
            "Random Forest (1000 rows) (F1: 0.8509, Precision: 0.8830, Recall: 0.8550)\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0030\n",
            "MLP (1000 rows) (F1: 0.9145, Precision: 0.9160, Recall: 0.9150)\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0495\n",
            "RNN (1000 rows) (F1: 0.9098, Precision: 0.9108, Recall: 0.9100)\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0028\n",
            "LSTM (1000 rows) (F1: 0.9344, Precision: 0.9370, Recall: 0.9350)\n",
            "SVM (2000 rows) (F1: 0.9249, Precision: 0.9249, Recall: 0.9250)\n",
            "Decision Tree (2000 rows) (F1: 1.0000, Precision: 1.0000, Recall: 1.0000)\n",
            "Random Forest (2000 rows) (F1: 0.9164, Precision: 0.9312, Recall: 0.9200)\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 8.4734e-04\n",
            "MLP (2000 rows) (F1: 0.9650, Precision: 0.9650, Recall: 0.9650)\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0091\n",
            "RNN (2000 rows) (F1: 0.9676, Precision: 0.9698, Recall: 0.9675)\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0017\n",
            "LSTM (2000 rows) (F1: 0.9602, Precision: 0.9629, Recall: 0.9600)\n",
            "SVM (3000 rows) (F1: 0.9316, Precision: 0.9317, Recall: 0.9317)\n",
            "Decision Tree (3000 rows) (F1: 1.0000, Precision: 1.0000, Recall: 1.0000)\n",
            "Random Forest (3000 rows) (F1: 0.9317, Precision: 0.9417, Recall: 0.9333)\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 5.4117e-04\n",
            "MLP (3000 rows) (F1: 0.9751, Precision: 0.9754, Recall: 0.9750)\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0046\n",
            "RNN (3000 rows) (F1: 0.9700, Precision: 0.9702, Recall: 0.9700)\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 7.7198e-04\n",
            "LSTM (3000 rows) (F1: 0.9749, Precision: 0.9750, Recall: 0.9750)\n",
            "SVM (4000 rows) (F1: 0.9384, Precision: 0.9386, Recall: 0.9387)\n",
            "Decision Tree (4000 rows) (F1: 1.0000, Precision: 1.0000, Recall: 1.0000)\n",
            "Random Forest (4000 rows) (F1: 0.8977, Precision: 0.9257, Recall: 0.9000)\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 3.5833e-04\n",
            "MLP (4000 rows) (F1: 0.9774, Precision: 0.9776, Recall: 0.9775)\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0027\n",
            "RNN (4000 rows) (F1: 0.9801, Precision: 0.9806, Recall: 0.9800)\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 4.5011e-04\n",
            "LSTM (4000 rows) (F1: 0.9799, Precision: 0.9801, Recall: 0.9800)\n",
            "SVM (5000 rows) (F1: 0.9269, Precision: 0.9273, Recall: 0.9270)\n",
            "Decision Tree (5000 rows) (F1: 1.0000, Precision: 1.0000, Recall: 1.0000)\n",
            "Random Forest (5000 rows) (F1: 0.9448, Precision: 0.9512, Recall: 0.9460)\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 1.0000 - loss: 3.1100e-04\n",
            "MLP (5000 rows) (F1: 0.9721, Precision: 0.9732, Recall: 0.9720)\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0015\n",
            "RNN (5000 rows) (F1: 0.9680, Precision: 0.9688, Recall: 0.9680)\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 3.5054e-04\n",
            "LSTM (5000 rows) (F1: 0.9729, Precision: 0.9735, Recall: 0.9730)\n",
            "\n",
            "Results by Data Size:\n",
            "   size       svm        dt        rf       mlp       rnn      lstm\n",
            "0   500  0.822382  1.000000  0.855118  0.910462  0.861949  0.949579\n",
            "1  1000  0.838484  0.990079  0.850859  0.914539  0.909844  0.934393\n",
            "2  2000  0.924881  1.000000  0.916353  0.965000  0.967648  0.960201\n",
            "3  3000  0.931636  1.000000  0.931697  0.975054  0.970036  0.974869\n",
            "4  4000  0.938387  1.000000  0.897689  0.977425  0.980053  0.979947\n",
            "5  5000  0.926932  1.000000  0.944770  0.972077  0.967996  0.972920\n",
            "\n",
            "Feature Impact Analysis (Random Forest):\n",
            "Features: ['Age', 'Gender', 'Height_cm', 'Weight_kg'], F1: 0.9679\n",
            "Features: ['Age', 'Gender', 'Height_cm', 'Weight_kg', 'Daily_Steps'], F1: 0.9619\n",
            "Features: ['Age', 'Gender', 'Height_cm', 'Weight_kg', 'Daily_Steps', 'Exercise_Frequency'], F1: 0.9456\n",
            "Features: ['Age', 'Gender', 'Height_cm', 'Weight_kg', 'Daily_Steps', 'Exercise_Frequency', 'Protein_Intake'], F1: 0.9658\n",
            "Features: ['Age', 'Gender', 'Height_cm', 'Weight_kg', 'Daily_Steps', 'Exercise_Frequency', 'Protein_Intake', 'Carbohydrate_Intake'], F1: 0.9587\n",
            "Features: ['Age', 'Gender', 'Height_cm', 'Weight_kg', 'Daily_Steps', 'Exercise_Frequency', 'Protein_Intake', 'Carbohydrate_Intake', 'Fat_Intake'], F1: 0.9558\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-1a7e43434b26>:267: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
            "  plt.tight_layout()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Random Forest Regression (Excluding Height/Weight):\n",
            "Random Forest Regression (MSE: 0.0013, R²: 1.0000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error, r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, Input\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "# Load the dataset\n",
        "df = pd.read_csv('nutrition_fitness_data.csv')\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nDescriptive Statistics:\\n\", df.describe())\n",
        "print(\"Columns in dataset:\", df.columns.tolist())\n",
        "\n",
        "# Calculate BMI and BMI categories\n",
        "df['bmi'] = df['Weight_kg'] / ((df['Height_cm'] / 100) ** 2)\n",
        "df['bmi_category'] = pd.cut(df['bmi'], bins=[0, 18.5, 24.9, 29.9, float('inf')],\n",
        "                            labels=['underweight', 'normal', 'overweight', 'obese'])\n",
        "\n",
        "# Define features and target for classification\n",
        "X = df.drop(['bmi', 'bmi_category'], axis=1)\n",
        "y = df['bmi_category'].cat.codes  # Convert to integers (0-3)\n",
        "\n",
        "# Create a small dataset (100 rows) for initial testing\n",
        "df_small = df.iloc[:100]\n",
        "X_small = df_small.drop(['bmi', 'bmi_category'], axis=1)\n",
        "y_small = df_small['bmi_category'].cat.codes\n",
        "\n",
        "# Split the data\n",
        "X_train_small, X_test_small, y_train_small, y_test_small = train_test_split(X_small, y_small, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define preprocessing pipeline\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())]), numeric_features),\n",
        "        ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_features)\n",
        "    ])\n",
        "\n",
        "# --- Model Evaluation Function ---\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name, save_cm=False):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test) if not isinstance(model, Sequential) else model.predict(X_test, verbose=0).argmax(axis=1)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    print(f\"{model_name} (F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f})\")\n",
        "    if save_cm:\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['underweight', 'normal', 'overweight', 'obese'])\n",
        "        disp.plot(cmap='Blues')\n",
        "        plt.title(f\"Confusion Matrix - {model_name}\")\n",
        "        plt.savefig(f\"cm_{model_name.lower().replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "    return f1\n",
        "\n",
        "# --- Model Training and Evaluation on Small Dataset (100 rows) ---\n",
        "print(\"\\nTesting on Small Dataset (100 rows):\")\n",
        "X_train_small_processed = preprocessor.fit_transform(X_train_small)\n",
        "X_test_small_processed = preprocessor.transform(X_test_small)\n",
        "\n",
        "# Prepare data for RNN/LSTM\n",
        "X_train_rnn_small = X_train_small_processed.reshape((X_train_small_processed.shape[0], 1, X_train_small_processed.shape[1]))\n",
        "X_test_rnn_small = X_test_small_processed.reshape((X_test_small_processed.shape[0], 1, X_test_small_processed.shape[1]))\n",
        "\n",
        "# Shallow models\n",
        "pipeline_logreg = Pipeline([('preprocessor', preprocessor), ('classifier', LogisticRegression(max_iter=1000))])\n",
        "pipeline_svm = Pipeline([('preprocessor', preprocessor), ('classifier', SVC())])\n",
        "pipeline_rf = Pipeline([('preprocessor', preprocessor), ('classifier', RandomForestClassifier(n_estimators=100))])\n",
        "pipeline_dt = Pipeline([('preprocessor', preprocessor), ('classifier', DecisionTreeClassifier())])\n",
        "\n",
        "f1_logreg = evaluate_model(pipeline_logreg, X_train_small, X_test_small, y_train_small, y_test_small, \"Logistic Regression\")\n",
        "f1_svm = evaluate_model(pipeline_svm, X_train_small, X_test_small, y_train_small, y_test_small, \"SVM\")\n",
        "f1_rf = evaluate_model(pipeline_rf, X_train_small, X_test_small, y_train_small, y_test_small, \"Random Forest\")\n",
        "f1_dt = evaluate_model(pipeline_dt, X_train_small, X_test_small, y_train_small, y_test_small, \"Decision Tree\")\n",
        "\n",
        "# MLP\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(200, 100, 50), max_iter=1000, random_state=42)\n",
        "f1_mlp = evaluate_model(mlp, X_train_small_processed, X_test_small_processed, y_train_small, y_test_small, \"MLP\")\n",
        "\n",
        "# RNN\n",
        "rnn_model_small = Sequential([\n",
        "    Input(shape=(1, X_train_small_processed.shape[1])),\n",
        "    SimpleRNN(100, activation='relu', return_sequences=True),\n",
        "    SimpleRNN(50, activation='relu', return_sequences=False),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "rnn_model_small.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "rnn_model_small.fit(X_train_rnn_small, y_train_small, epochs=10, batch_size=32, verbose=0)\n",
        "f1_rnn = evaluate_model(rnn_model_small, X_train_rnn_small, X_test_rnn_small, y_train_small, y_test_small, \"RNN\")\n",
        "\n",
        "# LSTM\n",
        "lstm_model_small = Sequential([\n",
        "    Input(shape=(1, X_train_small_processed.shape[1])),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "lstm_model_small.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model_small.fit(X_train_rnn_small, y_train_small, epochs=20, batch_size=32, verbose=0)\n",
        "f1_lstm = evaluate_model(lstm_model_small, X_train_rnn_small, X_test_rnn_small, y_train_small, y_test_small, \"LSTM\")\n",
        "\n",
        "# --- Model Training and Evaluation on Full Dataset ---\n",
        "print(\"\\nTesting on Full Dataset:\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "# Prepare data for RNN/LSTM\n",
        "X_train_rnn_full = X_train_processed.reshape((X_train_processed.shape[0], 1, X_train_processed.shape[1]))\n",
        "X_test_rnn_full = X_test_processed.reshape((X_test_processed.shape[0], 1, X_test_processed.shape[1]))\n",
        "\n",
        "# Shallow models\n",
        "f1_logreg_full = evaluate_model(LogisticRegression(max_iter=1000), X_train_processed, X_test_processed, y_train, y_test, \"Logistic Regression (Full)\")\n",
        "f1_svm_full = evaluate_model(SVC(), X_train_processed, X_test_processed, y_train, y_test, \"SVM (Full)\")\n",
        "f1_rf_full = evaluate_model(RandomForestClassifier(n_estimators=100), X_train_processed, X_test_processed, y_train, y_test, \"Random Forest (Full)\", save_cm=True)\n",
        "f1_dt_full = evaluate_model(DecisionTreeClassifier(), X_train_processed, X_test_processed, y_train, y_test, \"Decision Tree (Full)\")\n",
        "\n",
        "# MLP\n",
        "f1_mlp_full = evaluate_model(MLPClassifier(hidden_layer_sizes=(200, 100, 50), max_iter=1000), X_train_processed, X_test_processed, y_train, y_test, \"MLP (Full)\")\n",
        "\n",
        "# RNN\n",
        "rnn_model_full = Sequential([\n",
        "    Input(shape=(1, X_train_processed.shape[1])),\n",
        "    SimpleRNN(100, activation='relu', return_sequences=True),\n",
        "    SimpleRNN(50, activation='relu', return_sequences=False),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "rnn_model_full.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "rnn_model_full.fit(X_train_rnn_full, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "f1_rnn_full = evaluate_model(rnn_model_full, X_train_rnn_full, X_test_rnn_full, y_train, y_test, \"RNN (Full)\", save_cm=True)\n",
        "\n",
        "# LSTM\n",
        "lstm_model_full = Sequential([\n",
        "    Input(shape=(1, X_train_processed.shape[1])),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "lstm_model_full.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "lstm_model_full.fit(X_train_rnn_full, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "f1_lstm_full = evaluate_model(lstm_model_full, X_train_rnn_full, X_test_rnn_full, y_train, y_test, \"LSTM (Full)\")\n",
        "\n",
        "# --- Model Performance Across Data Sizes ---\n",
        "print(\"\\nTesting Model Performance Across Data Sizes:\")\n",
        "sizes = [500, 1000, 2000, 3000, 4000, 5000]\n",
        "results = {'size': [], 'svm': [], 'dt': [], 'rf': [], 'mlp': [], 'rnn': [], 'lstm': []}\n",
        "\n",
        "for size in sizes:\n",
        "    X_sample = X.sample(n=size, random_state=42)\n",
        "    y_sample = y[X_sample.index]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "    X_train_rnn = X_train_processed.reshape((X_train_processed.shape[0], 1, X_train_processed.shape[1]))\n",
        "    X_test_rnn = X_test_processed.reshape((X_test_processed.shape[0], 1, X_test_processed.shape[1]))\n",
        "\n",
        "    # Evaluate models\n",
        "    svm_f1 = evaluate_model(SVC(), X_train_processed, X_test_processed, y_train, y_test, f\"SVM ({size} rows)\")\n",
        "    dt_f1 = evaluate_model(DecisionTreeClassifier(), X_train_processed, X_test_processed, y_train, y_test, f\"Decision Tree ({size} rows)\")\n",
        "    rf_f1 = evaluate_model(RandomForestClassifier(n_estimators=100), X_train_processed, X_test_processed, y_train, y_test, f\"Random Forest ({size} rows)\")\n",
        "    mlp_model = Sequential([\n",
        "        Input(shape=(X_train_processed.shape[1],)),\n",
        "        Dense(200, activation='relu'),\n",
        "        Dense(100, activation='relu'),\n",
        "        Dense(50, activation='relu'),\n",
        "        Dense(4, activation='softmax')\n",
        "    ])\n",
        "    mlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    mlp_model.fit(X_train_processed, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "    mlp_f1 = evaluate_model(mlp_model, X_train_processed, X_test_processed, y_train, y_test, f\"MLP ({size} rows)\")\n",
        "    rnn_model = Sequential([\n",
        "        Input(shape=(1, X_train_processed.shape[1])),\n",
        "        SimpleRNN(100, return_sequences=True),\n",
        "        SimpleRNN(50),\n",
        "        Dense(4, activation='softmax')\n",
        "    ])\n",
        "    rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    rnn_model.fit(X_train_rnn, y_train, epochs=10, batch_size=32, verbose=0)\n",
        "    rnn_f1 = evaluate_model(rnn_model, X_train_rnn, X_test_rnn, y_train, y_test, f\"RNN ({size} rows)\")\n",
        "    lstm_model = Sequential([\n",
        "        Input(shape=(1, X_train_processed.shape[1])),\n",
        "        LSTM(128, return_sequences=True),\n",
        "        LSTM(128, return_sequences=True),\n",
        "        LSTM(32),\n",
        "        Dense(4, activation='softmax')\n",
        "    ])\n",
        "    lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    lstm_model.fit(X_train_rnn, y_train, epochs=20, batch_size=32, verbose=0)\n",
        "    lstm_f1 = evaluate_model(lstm_model, X_train_rnn, X_test_rnn, y_train, y_test, f\"LSTM ({size} rows)\")\n",
        "\n",
        "    # Store results\n",
        "    results['size'].append(size)\n",
        "    results['svm'].append(svm_f1)\n",
        "    results['dt'].append(dt_f1)\n",
        "    results['rf'].append(rf_f1)\n",
        "    results['mlp'].append(mlp_f1)\n",
        "    results['rnn'].append(rnn_f1)\n",
        "    results['lstm'].append(lstm_f1)\n",
        "\n",
        "# Save and plot results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('results_by_size.csv', index=False)\n",
        "print(\"\\nResults by Data Size:\")\n",
        "print(results_df)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for col in ['svm', 'dt', 'rf', 'mlp', 'rnn', 'lstm']:\n",
        "    plt.plot(results['size'], results[col], marker='o', label=col.upper())\n",
        "plt.xlabel('Data Size (Rows)', fontsize=12)\n",
        "plt.ylabel('F1 Score', fontsize=12)\n",
        "plt.title('Model Performance vs Data Size', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True)\n",
        "plt.ylim(0.8, 1.0)\n",
        "plt.savefig('results_chart.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- Feature Impact Analysis (Random Forest) ---\n",
        "print(\"\\nFeature Impact Analysis (Random Forest):\")\n",
        "initial_features = ['Age', 'Gender', 'Height_cm', 'Weight_kg']\n",
        "additional_features = ['Daily_Steps', 'Exercise_Frequency', 'Protein_Intake', 'Carbohydrate_Intake', 'Fat_Intake']\n",
        "feature_subsets = [initial_features]\n",
        "for feat in additional_features:\n",
        "    feature_subsets.append(feature_subsets[-1] + [feat])\n",
        "\n",
        "f1_scores = []\n",
        "for subset in feature_subsets:\n",
        "    X_subset = df[subset]\n",
        "    subset_numeric = X_subset.select_dtypes(include=['float64', 'int64']).columns\n",
        "    subset_categorical = X_subset.select_dtypes(include=['object']).columns\n",
        "    subset_preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', Pipeline([('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())]), subset_numeric),\n",
        "            ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), subset_categorical)\n",
        "        ])\n",
        "    X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_subset, y, test_size=0.2, random_state=42)\n",
        "    X_train_sub_processed = subset_preprocessor.fit_transform(X_train_sub)\n",
        "    X_test_sub_processed = subset_preprocessor.transform(X_test_sub)\n",
        "    rf_subset = RandomForestClassifier(n_estimators=100)\n",
        "    rf_subset.fit(X_train_sub_processed, y_train_sub)\n",
        "    y_pred_sub = rf_subset.predict(X_test_sub_processed)\n",
        "    f1_sub = f1_score(y_test_sub, y_pred_sub, average='weighted')\n",
        "    f1_scores.append(f1_sub)\n",
        "    print(f\"Features: {subset}, F1: {f1_sub:.4f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(feature_subsets)), f1_scores, marker='o')\n",
        "plt.xticks(range(len(feature_subsets)), [str(subset) for subset in feature_subsets], rotation=45)\n",
        "plt.xlabel('Feature Subset')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.title('Feature Impact on Random Forest F1-Score')\n",
        "plt.tight_layout()\n",
        "plt.savefig('feature_impact_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- Data Visualizations ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df['Age'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Age')\n",
        "plt.subplot(1, 2, 2)\n",
        "df['Gender'].value_counts().plot(kind='bar', color='lightgreen')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Gender')\n",
        "plt.tight_layout()\n",
        "plt.savefig('dist_age_gender.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.scatterplot(x='Age', y='bmi', hue='Gender', data=df)\n",
        "plt.title('BMI vs. Age by Gender')\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(x='bmi_category', y='Protein_Intake', data=df)\n",
        "plt.title('Protein Intake by BMI Category')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('bmi_age_protein.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(x='Chronic_Disease', data=df)\n",
        "plt.title('Distribution of Chronic Diseases')\n",
        "plt.xlabel('Chronic Disease')\n",
        "plt.ylabel('Count')\n",
        "plt.savefig('dist_chronic_disease.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "df['Preferred_Cuisine'].value_counts().plot(kind='bar', color='salmon')\n",
        "plt.xlabel('Preferred Cuisine')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Preferred Cuisine')\n",
        "plt.savefig('dist_preferred_cuisine.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "corr_matrix = df[numeric_features].corr()\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', mask=np.triu(corr_matrix))\n",
        "plt.title('Correlation Matrix')\n",
        "plt.savefig('corr_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# --- Random Forest Regression (Excluding Height/Weight) ---\n",
        "print(\"\\nRandom Forest Regression (Excluding Height/Weight):\")\n",
        "X_reg = df.drop(['bmi', 'bmi_category', 'Height_cm', 'Weight_kg'], axis=1)\n",
        "y_reg = df['bmi']\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "numeric_features_reg = X_reg.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features_reg = X_reg.select_dtypes(include=['object']).columns\n",
        "preprocessor_reg = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())]), numeric_features_reg),\n",
        "        ('cat', Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), categorical_features_reg)\n",
        "    ])\n",
        "\n",
        "X_train_reg_processed = preprocessor_reg.fit_transform(X_train_reg)\n",
        "X_test_reg_processed = preprocessor_reg.transform(X_test_reg)\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=100)\n",
        "rf_reg.fit(X_train_reg_processed, y_train_reg)\n",
        "y_pred_reg = rf_reg.predict(X_test_reg_processed)\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "print(f\"Random Forest Regression (MSE: {mse:.4f}, R²: {r2:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNY50VBF7IctIEjjdp2NCk5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}